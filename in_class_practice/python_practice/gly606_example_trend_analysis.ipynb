{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8749920-d23c-4f10-b71a-74d368a94e50",
   "metadata": {},
   "source": [
    "# Examples for Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9edff-2d05-4e52-bfd5-8effb124c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb68c2a-6ec7-460a-9458-556c12e67989",
   "metadata": {},
   "source": [
    "## 1. Data preparation - Here we use `St Lawrence River` as an example\n",
    "\n",
    "In this section, we downloaded USGS data from St Lawrence River."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168db3e-fced-4cbd-a1e5-44f4bba8a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'site_no': '04264331',\n",
    "    'begin_date': '1936-01-01',\n",
    "    'end_date': '2022-12-31'\n",
    "}\n",
    "query = urllib.parse.urlencode(args)\n",
    "verde_url = (\n",
    "    f'https://waterdata.usgs.gov/nwis/dv?'\n",
    "    f'cb_00060=on&format=rdb&referred_module=sw&{query}'\n",
    ")\n",
    "response = urllib.request.urlopen(verde_url)\n",
    "df1 = pd.read_table(\n",
    "    response,\n",
    "    comment='#',\n",
    "    sep='\\s+',\n",
    "    names=['agency', 'site', 'date', 'streamflow', 'quality_flag'],\n",
    "    index_col=2,\n",
    "    parse_dates=True,\n",
    "    date_format='yyyy-mm-dd',\n",
    "    engine='python')\n",
    "# discard the first two rows\n",
    "df1 = df1.iloc[2:]\n",
    "# Now convert the streamflow data to floats and\n",
    "# the index to datetimes. When processing raw data\n",
    "# it's common to have to do some extra postprocessing\n",
    "df1['streamflow'] = df1['streamflow'].astype(np.float64)\n",
    "df1.index = pd.DatetimeIndex(df1.index)\n",
    "\n",
    "# we calculated the annual peak flow for the St Lawrence River\n",
    "st_law_peakflow = df1[['streamflow']].groupby(df1.index.year).max()\n",
    "st_law_peakflow.columns = ['peakflow_cfs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25a389-dc5e-4a04-8449-837f61b95768",
   "metadata": {},
   "source": [
    "# 2. Linear regression\n",
    "\n",
    "It is mostly illustration. It contains a total number of **two** practices in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3fb767-ef15-4da1-a5f6-7b0ac1d1c7e3",
   "metadata": {},
   "source": [
    "## 2.1. Least Squared Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22960fd-df5b-4370-a644-3595fc2fdf2f",
   "metadata": {},
   "source": [
    "**St Lawrence River Example: Whether we can use annual mean flow to predict the annual peak flow?**\n",
    "\n",
    "As a continuation from Problem 1, we look at the St Lawrence River. The annual peak flow was calculated above with the variable name `st_law_peakflow`. We first calculate the annual mean flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8fb6a-5716-474d-9a07-acc94414d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_law_mean_flow = df1[['streamflow']].groupby(df1.index.year).mean()\n",
    "st_law_mean_flow.columns = ['meanflow_cfs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08395409-3c19-4e64-9062-fccb27da61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the annual peakflow data and annual mean data\n",
    "# in one file for convenience\n",
    "st_law_flow_df = pd.concat([st_law_peakflow,st_law_mean_flow],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83afd9-2e0e-4b81-bf5d-6a11a934880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add years column for plotting convenience\n",
    "st_law_flow_df['year'] = st_law_flow_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbe17a-d11f-4fcc-8f28-aa80829f4029",
   "metadata": {},
   "source": [
    "#### Practice #1: Plot the time series of Annual Peak Flow and Annual Mean flow\n",
    "Please plot both time series in one figure and assign different colors to them </br>\n",
    "Plot type: Line plot </br>\n",
    "Colors: Orange for Annual Mean Flow, and Blue or Annual Peak Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cc9b9-a841-4626-83b6-0e1234902e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7886256-bcc7-445c-b211-85c23dcdb863",
   "metadata": {},
   "source": [
    "#### What does the above plot show?\n",
    "\n",
    "What you see above is a plot of the time series of annual mean flow (orange line) and annual peak flow (blue line). For a year with more water availability (annual mean flow), we might expect a high peak flow as well. We can check this by examining a regression between the annual mean flow and annual peak flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8044e25-4b99-42fb-8300-5b5d995a7c8e",
   "metadata": {},
   "source": [
    "### 2.1.1. The first step to any regression or correlation analysis is to create a scatter plot of the data.\n",
    "\n",
    "#### Practice #2: Please generate the scatter plot. \n",
    "\n",
    "* Please add the xlabel \"Annual Mean Flow [cfs]\", and ylabel \"Annual Peak Flow [cfs]\"\n",
    "* Please add the title \"Streamflow at St Lawrence River, NY \\n1936-2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788da1cd-382b-4bfb-8150-180434babb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd5f7f-99aa-4f21-8604-c340d69c445d",
   "metadata": {},
   "source": [
    "#### Linear regression: Could we use Annual Mean flow to predict Annual Peak Flow?\n",
    "\n",
    "The plot above suggests that this is a borderline case for applying linear regression analysis. What rules of linear regression might we worry about here? (heteroscedasticity)\n",
    "\n",
    "We will proceed with calculating the regression and then look at the residuals to get a better idea of whether this is the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef2f1f-3faf-4ab0-818d-737c6028eb9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.1.2. Manual calculation of linear regression\n",
    "\n",
    "Here we'll first compute it manually, solving for our y-intercept, $B_0$, and slope $B_1$:\n",
    "\n",
    "$B_1 = \\displaystyle \\frac{n(\\sum_{i=1}^{n}x_iy_i)-(\\sum_{i=1}^{n}x_i)(\\sum_{i=1}^{n}y_i)}{n(\\sum_{i=1}^{n}x_i^2)-(\\sum_{i=1}^{n}x_i)^2}$\n",
    "\n",
    "$B_0 = \\displaystyle \\frac{(\\sum_{i=1}^{n}y_i)-B_1(\\sum_{i=1}^{n}x_i)}{n} = \\bar{y} - B_1\\bar{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdf335-2479-4005-b8a0-669222141e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(st_law_flow_df) # length of our dataset\n",
    "\n",
    "x = st_law_flow_df.meanflow_cfs # using x for shorthand below\n",
    "y = st_law_flow_df.peakflow_cfs # using y for shorthand below\n",
    "\n",
    "B1 = ( n*np.sum(x*y) - np.sum(x)*np.sum(y) ) / ( n*np.sum(x**2) - np.sum(x)**2 ) # B1 parameter, slope\n",
    "B0 = np.mean(y) - B1*np.mean(x) # B0 parameter, y-intercept\n",
    "\n",
    "print('B0 : {}'.format(np.round(B0,4)))\n",
    "print('B1 : {}'.format(np.round(B1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807a307-4acb-4fd0-9304-f26a04311793",
   "metadata": {},
   "source": [
    "Then our linear model to predict $y$ at each $x_i$ is: $\\hat{y}_i = B_0 + B_1x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b01aa-a229-4b52-9ec2-bea7b93d51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = B0 + B1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5e485-a159-4db8-a03c-8fb7ac612556",
   "metadata": {},
   "source": [
    "And our residuals are: $(y_i - \\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40e82f-412e-4852-8848-7d86d45d28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y - y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8f999-41b7-4c66-9e2e-eff06cd76ae8",
   "metadata": {},
   "source": [
    "Finally, compute our Sum of Squared Errors (from our residuals) and Total Sum of Squares to get the correlation coefficient, R, for this linear model.\n",
    "\n",
    "$SSE = \\displaystyle\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ \n",
    "\n",
    "$SST = \\displaystyle\\sum_{i=1}^{n} (y_i - \\bar{y}_i)^2$\n",
    "\n",
    "$R^2 = 1 - \\displaystyle \\frac{SSE}{SST}$\n",
    "\n",
    "And compute the standard error of the estimate, $\\sigma$ for this model.\n",
    "\n",
    "$\\sigma = \\sqrt{\\displaystyle\\frac{SSE}{(n-2)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d2525-4317-4bfe-be11-f65eee111eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = np.sum(residuals**2)\n",
    "\n",
    "sst = np.sum( (y - np.mean(y))**2 )\n",
    "\n",
    "r_squared = 1 - sse/sst\n",
    "r = np.sqrt(r_squared)\n",
    "\n",
    "s = np.sqrt(sse/(n-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e746b-de33-4ca1-8233-6020af94d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SSE : {} cfs'.format(np.round(sse,2)))\n",
    "print('SST : {} cfs'.format(np.round(sst,2)))\n",
    "print('R^2 : {}'.format(np.round(r_squared,3)))\n",
    "print('R : {}'.format(np.round(r,3)))\n",
    "print('sigma : {}'.format(np.round(s,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09223913-66e5-47f0-95c8-4dd401581398",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2, ax3] = plt.subplots(nrows=1, ncols=3, figsize=(14,4), tight_layout=True)\n",
    "\n",
    "# Scatterplot\n",
    "st_law_flow_df.plot.scatter(x='meanflow_cfs', y='peakflow_cfs', c='k', ax=ax1);\n",
    "\n",
    "# Plot the regression line, we only need two points to define a line, use xmin and xmax\n",
    "ax1.plot([x.min(), x.max()], [B0 + B1*x.min(), B0 + B1*x.max()] , '-r')\n",
    "\n",
    "ax1.set_xlabel('Annual Mean Flow (cfs)')\n",
    "ax1.set_ylabel('Annual Peak Flow (cfs)');\n",
    "\n",
    "# ax1.set_xlim((0,3000))\n",
    "# ax1.set_ylim((0,1000));\n",
    "\n",
    "# Plot the residuals\n",
    "ax2.plot(st_law_flow_df.year,residuals,'-o')\n",
    "\n",
    "ax2.set_xlabel('Years')\n",
    "ax2.set_ylabel('Residuals, SWE (mm)');\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "ax3.hist(residuals, bins=10)\n",
    "\n",
    "ax3.set_xlabel('Residuals, SWE (mm)')\n",
    "ax3.set_ylabel('Number of Data Points');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970725e9-6b3a-40db-9649-d57139cda07a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Linear regression using the scipy library\n",
    "\n",
    "Now we'll use the `scipy.stats.linregress()` function to do the same thing. Review the documentation or help text for this function before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30195aa1-6096-4959-8c92-c04b9238ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.linregress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d55375-baf5-4c65-88fa-a5f5f9f3af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the linear regression function\n",
    "slope, intercept, rvalue, pvalue, stderr = stats.linregress(st_law_flow_df.meanflow_cfs, \n",
    "                                                            st_law_flow_df.peakflow_cfs)\n",
    "\n",
    "print('B0 : {}'.format(np.round(intercept,4)))\n",
    "print('B1 : {}'.format(np.round(slope,4)))\n",
    "\n",
    "print('R^2 : {}'.format(np.round(rvalue**2,3)))\n",
    "print('R : {}'.format(np.round(rvalue,3)))\n",
    "print('stderr : {}'.format(np.round(stderr,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da1b2a-5f9c-4b99-bf6f-a15039caa9d8",
   "metadata": {},
   "source": [
    "Do we get the same results as above?\n",
    "\n",
    "No, our \"standard error\" is different. Why is that? If you look into the documentation for the lingregress function, you'll see that it calls this output the \"standard error of the **gradient**\" meaning the standard error of the slope, $B1$.\n",
    "\n",
    "This is related to the \"standard error\", $\\sigma$ like:\n",
    "\n",
    "$SE_{B_1} = \\displaystyle \\frac{\\sigma}{\\sqrt{SST_x}} $ where $SST_x = \\displaystyle\\sum_{i=1}^{n} (x_i - \\bar{x}_i)^2$\n",
    "\n",
    "Compute the standard error from the standard error of the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b378f4e-2bdd-41a6-bc25-6f371139407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the SST for x\n",
    "sst_x = np.sum( (x - np.mean(x))**2 )\n",
    "\n",
    "# Compute the standard error\n",
    "sigma = stderr * np.sqrt(sst_x)\n",
    "print('sigma : {}'.format(np.round(sigma,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea62cb-8258-422f-a7a5-73b1763fc51b",
   "metadata": {},
   "source": [
    "This should now match what we solved for manually above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020372d-afe7-4eca-8a2f-ca763b0aeadf",
   "metadata": {},
   "source": [
    "Finally, plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c8207-3f5d-4e2e-9fc5-6ff7eac96fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6),dpi=200)\n",
    "\n",
    "# Scatterplot\n",
    "st_law_flow_df.plot.scatter(x='meanflow_cfs', y='peakflow_cfs', c='k', ax=ax);\n",
    "\n",
    "# Create points for the regression line\n",
    "x_1 = np.linspace(st_law_flow_df.meanflow_cfs.min(), \n",
    "                st_law_flow_df.meanflow_cfs.max(), 2) # make two x coordinates from min and max values of SLI_max\n",
    "y_1 = slope * x_1 + intercept # y coordinates using the slope and intercept from our linear regression to draw a regression line\n",
    "\n",
    "# Plot the regression line\n",
    "ax.plot(x_1, y_1, '-r')\n",
    "\n",
    "ax.set_xlabel(\"Annual Mean Flow [cfs]\")\n",
    "ax.set_ylabel(\"Annual Peak Flow [cfs]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d3443-8397-42a1-abc0-3c1fcec27d00",
   "metadata": {},
   "source": [
    "We've used the slope and intercept from the linear regression, what were the other values the function returned to us?\n",
    "\n",
    "This function gives us our R value, we can report how well our linear regression fits our data with this or R-squared (you can see in this case linear regression did a poor job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37ba88-8aed-4cea-a48e-d4ed96200b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r-value = {}'.format(rvalue))\n",
    "\n",
    "print('r-squared = {}'.format(rvalue**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fb1c2-28e7-4cf0-9c78-de18ec3a6838",
   "metadata": {},
   "source": [
    "This function also performed a two-sided \"Wald Test\" (t-distribution) to test if the slope of the linear regression is different from zero (null hypothesis is that the slope is not different from a slope of zero). Be careful using this default statistical test though, is this the test that you really need to use on your data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8c1bb-1ff5-483b-8895-70604775589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('p-value = {}'.format(pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7615b0f-887c-4fa9-9013-7720984aa24e",
   "metadata": {},
   "source": [
    "And finally it gave us the standard error of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083c48e-3d9e-484a-9e1e-3a5c12ac19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('standard error = {}'.format(stderr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fae2e8-d906-4978-9843-329a094e3796",
   "metadata": {},
   "source": [
    "Now use this linear model to predict a $y$ (Annual Peak Flow) value for each $x$ (Annual Mean Flow) value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839c030-2e4a-4f91-a580-17281656acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = slope * st_law_flow_df.meanflow_cfs + intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0307ea3-a299-4355-9807-33b794168a48",
   "metadata": {},
   "source": [
    "**Plot residuals**\n",
    "\n",
    "We should make a plot of the residuals (actual - predicted values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f768f-6497-4f64-9d19-3736e2330ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = st_law_flow_df.peakflow_cfs - y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97c599-b4a1-4308-bf3e-b6897f0ce26d",
   "metadata": {},
   "source": [
    "For a good linear fit, we hope that our residuals are small, don't have any trends or patterns themselves, want them to be normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47388a32-94e4-4ac9-9b7e-17d2e352f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1,2,figsize=(9,4),dpi=200)\n",
    "\n",
    "ax1.plot(st_law_flow_df.year,residuals)\n",
    "ax1.set_xlabel('years')\n",
    "ax1.set_ylabel('residuals (cfs)')\n",
    "\n",
    "ax2.hist(residuals)\n",
    "ax2.set_xlabel('residuals (cfs)')\n",
    "ax2.set_ylabel('count')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c7fc2-f780-479d-a876-2abab4ba737e",
   "metadata": {},
   "source": [
    "## 2.2. Confidence Interval for the Slope (B1)\n",
    "\n",
    "**Compute the confidence intervals around our B1 parameter, the slope**\n",
    "\n",
    "We first specify our $\\alpha$ for our chosen level of confidence (95%), and our degrees of freedom $dof = n - 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25791b-c1b5-4f64-9b85-5e3c9a3d5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our alpha for 95% confidence\n",
    "alpha = 0.05\n",
    "\n",
    "# length of the dataset\n",
    "n = len(x)\n",
    "print(n)\n",
    "# degrees of freedom\n",
    "dof = n - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e680907-8058-4f2a-8d66-77e31280a4cd",
   "metadata": {},
   "source": [
    "Now, compute the Standard Error of the Gradient (Slope):\n",
    "\n",
    "$s_{B_1} = \\displaystyle \\frac{s}{\\sqrt{SST_x}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd48cf-aef8-4525-a700-9cbc13fff23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard error of the gradient (slope)\n",
    "sB1 = s/np.sqrt(sst_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bdb21-7a7f-423f-ab31-a8386180e7d4",
   "metadata": {},
   "source": [
    "This follows a t-distribution, find the t-value that corresponds with our $\\alpha$ and $dof$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67177aeb-bc42-46d5-b3fc-7d469bb47867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-value for alpha/2 with n-2 degrees of freedom\n",
    "t = stats.t.ppf(1-alpha/2, dof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec48b62-8e15-4b83-a912-a505af5376ed",
   "metadata": {},
   "source": [
    "Compute the upper and lower limits for the B1 parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c2a83-805e-4e72-b1cf-6930baf9f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the upper and lower limits on our B1 (slope) parameter\n",
    "B1_upper = B1 + t * sB1\n",
    "B1_lower = B1 - t * sB1\n",
    "\n",
    "# compute the corresponding upper and lower B0 values (y intercepts)\n",
    "B0_upper = y.mean() - B1_upper*x.mean()\n",
    "B0_lower = y.mean() - B1_lower*x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5d09e-d023-4f67-9e16-b135d6385d5f",
   "metadata": {},
   "source": [
    "**Plot the data, linear regression model, and confidence intervals for B1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c4775-921d-4377-a1b0-3f75c54c63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7), dpi=200, tight_layout=True)\n",
    "\n",
    "# Scatterplot of original data\n",
    "ax.scatter(x, y, c='k', label='Original Data')\n",
    "\n",
    "# Plot the regression line, we only need two points to define a line, use xmin and xmax\n",
    "ax.plot([x.min(), x.max()], [B0 + B1*x.min(), B0 + B1*x.max()] , '-r', label='Least Squares Linear Regression Model')\n",
    "\n",
    "# Plot the mean line, we only need two points to define a line, use xmin and xmax\n",
    "ax.plot([x.min(), x.max()], [y.mean(), y.mean()] , '--m', label='Mean Y')\n",
    "\n",
    "# Plot the upper and lower confidence limits for the standard error of the gradient (slope)\n",
    "ax.plot([x.min(), x.max()], [B0_upper + B1_upper*x.min(), B0_upper + B1_upper*x.max()] , '--r', label='Upper B0 confidence limit (95%)')\n",
    "ax.plot([x.min(), x.max()], [B0_lower + B1_lower*x.min(), B0_lower + B1_lower*x.max()] , '--r', label='Upper B0 confidence limit (95%)')\n",
    "\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='lower right');\n",
    "\n",
    "# Add axes labels and title\n",
    "ax.set_xlabel(\"Annual Mean Flow [cfs]\")\n",
    "ax.set_ylabel(\"Annual Peak Flow [cfs]\")\n",
    "ax.set_title('Linear Regression Model with Confidence Intervals');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4eec5-6ab0-4ea2-b47d-a55d5f95e4be",
   "metadata": {},
   "source": [
    "## 2.3. Confidence Interval for Predicted Values of y\n",
    "\n",
    "**Compute confidence limits for the predicted values of y**\n",
    "\n",
    "To compute confidence limits on our predicted values of y, we need to predict some values of y first!\n",
    "\n",
    "For the prediction intervals, I'm naming the variables `p_x` and `p_y`, in the equations below these correspond to $x^*$ and $\\hat{y}^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58c4be-7684-440c-a3cc-104277e4bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an array of x values\n",
    "p_x = np.linspace(x.min(),x.max(),100)\n",
    "\n",
    "# using our model parameters to predict y values\n",
    "p_y = B0 + B1*p_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca29b4a-4a64-47de-9d39-f592a78ba3d7",
   "metadata": {},
   "source": [
    "For some value $x^*$ we want to predict a corresponding $y^*$ using our model.\n",
    "\n",
    "$\\hat{y}^* = \\hat{B}_0 + \\hat{B}_1x^*$\n",
    "\n",
    "But what is the undercertainty of the $\\hat{y}^*$ we'll calculate? We can compute a prediction interval for a given confidence (such a 95%).\n",
    "\n",
    "The error of our prediction is the difference between the \"true\" value of $y^*$ for $x^*$, and our predicted $\\hat{y}^*$:\n",
    "\n",
    "$B_0 + B_1x^* - \\hat{B}_0 + \\hat{B}_1x^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861cc77-093f-4eb7-9686-12d379405a5b",
   "metadata": {},
   "source": [
    "The variance of this prediction error ($\\sigma_{E_P}^2$) will help define our prediction intervals, and can be computed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafa74f-f591-4e22-b21a-5d381c8b76f3",
   "metadata": {},
   "source": [
    "$\\sigma_{E_p}^2(x^*) = s^2 \\Bigg[ 1 + \\displaystyle\\frac{1}{n} + \\displaystyle\\frac{n(x^*-\\bar{x})^2}{n \\sum{x_i^2} + (\\sum{x_i})^2} \\Bigg]$\n",
    "\n",
    "or\n",
    "\n",
    "$\\sigma_{E_p}^2(x^*) = s^2 \\Bigg[ 1 + \\displaystyle\\frac{1}{n} + \\displaystyle\\frac{(x^*-\\bar{x})^2}{SST_x} \\Bigg]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48530c18-f177-4bc4-8b73-5cbbf157344e",
   "metadata": {},
   "source": [
    "Now compute our error of prediction ($\\sigma_{E_p}$) for each p_x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db74ed-0e95-4ce4-b5cc-2f53b0aecb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_ep = np.sqrt( s**2 * (1+ 1/n + ( ( n*(p_x-x.mean())**2 ) / \n",
    "                                      ( n*np.sum(x**2) - np.sum(x)**2 ) ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935a270-42a6-4ed9-aba5-67e5f706ab71",
   "metadata": {},
   "source": [
    "The lower and upper confidence limits based on predicted y and confidence intervals (which follow a t-distribution) can be computed as:\n",
    "\n",
    "$y^* \\pm t_{\\frac{\\alpha}{2},n-2} \\cdot \\sigma_{E_p}(x^*)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2c66b-3fcc-4c24-bc0b-a5a2debdde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "n = len(p_x)\n",
    "dof = n - 2\n",
    "\n",
    "t = stats.t.ppf(1-alpha/2, dof)\n",
    "\n",
    "p_y_lower = p_y - t * sigma_ep\n",
    "p_y_upper = p_y + t * sigma_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1546f5-d136-44e4-9666-2a6cc46d2766",
   "metadata": {},
   "source": [
    "**Finally, plot the upper and lower confidence limits for the predicted y values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c520c1-081b-4240-923b-5f754ab87117",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7), dpi=300, tight_layout=True)\n",
    "\n",
    "# Scatterplot of original data\n",
    "ax.scatter(x, y, c='k', label='Original Data')\n",
    "\n",
    "# Plot the regression line, we only need two points to define a line, use xmin and xmax\n",
    "ax.plot([x.min(), x.max()], [B0 + B1*x.min(), B0 + B1*x.max()] , '-r', label='Least Squares Linear Regression Model')\n",
    "\n",
    "# Plot the mean line, we only need two points to define a line, use xmin and xmax\n",
    "ax.plot([x.min(), x.max()], [y.mean(), y.mean()] , '--m', label='Mean Y')\n",
    "\n",
    "# Plot the mean x line\n",
    "plt.axvline(x.mean(),c='k', linestyle='--', label='Mean X Value')\n",
    "\n",
    "# Plot the upper and lower confidence limits for the standard error of the gradient (slope)\n",
    "ax.plot([x.min(), x.max()], [B0_upper + B1_upper*x.min(), B0_upper + B1_upper*x.max()] , '--r', label='Upper B0 confidence limit (95%)')\n",
    "ax.plot([x.min(), x.max()], [B0_lower + B1_lower*x.min(), B0_lower + B1_lower*x.max()] , '--r', label='Upper B0 confidence limit (95%)')\n",
    "\n",
    "# Plot confidence limits on our predicted Y values\n",
    "ax.plot(p_x, p_y_upper, ':b', label='Upper Y prediction interval (95%)')\n",
    "ax.plot(p_x, p_y_lower, ':b', label='Lower Y prediction interval (95%)')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='lower right');\n",
    "\n",
    "# Add axes labels and title\n",
    "ax.set_xlabel(\"Annual Mean Flow [cfs]\")\n",
    "ax.set_ylabel(\"Annual Peak Flow [cfs]\")\n",
    "ax.set_title('Linear Regression Model with Confidence Intervals');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb9784-b26a-43ac-82b9-febfd2797caa",
   "metadata": {},
   "source": [
    "Our upper and lower predicted y confidence limits look almost parallel, but are they? \n",
    "\n",
    "To inspect this, we can plot the difference between the two versus x to see how our 95% interval changes shape as we move along the x axis, and see that they \"pivot\" around the mean x value of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d8df7-0432-4c85-b9ab-f771139f011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_y_difference = p_y_upper - p_y_lower\n",
    "plt.figure(figsize=[5,3],dpi=300)\n",
    "plt.plot(p_x, p_y_difference, label='p_y_difference')\n",
    "plt.axvline(x.mean(),c='k', linestyle='--', label='Mean X Value')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Prediction Input (Annual Mean Flow, cfs)')\n",
    "plt.ylabel('Difference Between Upper and Lower\\nY Prediction Confidence Bounds ($\\Delta$cfs)')\n",
    "plt.title('Difference Between Upper and Lower\\nY Prediction 95% Confidence Bounds');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c581747-270c-4990-a102-8c2005794a87",
   "metadata": {},
   "source": [
    "As we'd expect, they're not quite parallel (they vary along the x-axis) and are narrowest at $\\bar{x}$ where we have higher confidence in our ability to make predictions with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4f977-4571-4bee-a5db-367f92b67a7b",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.4. Linear regression with scipy\n",
    "\n",
    "**How do we do this quickly in python?**\n",
    "\n",
    "As always, there are a few options, two of the easier ones that are in packages we already have here are:\n",
    "- `scipy.stats.linregress()` we've used this previously\n",
    "- `numpy.polyfit()` we can fit a 1st order polynomial (linear function)\n",
    "\n",
    "I'm going to use the scipy function below (remember, this outputs our standard error of the gradient for us already):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf792c-921f-4b99-9751-5fceefc01185",
   "metadata": {},
   "outputs": [],
   "source": [
    "B1, B0, r, p, sB1 = stats.linregress(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4472c06-f41f-4669-a08a-aa320799eea4",
   "metadata": {},
   "source": [
    "Compute the upper and lower limits for the B1 parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f52210-5766-4a07-8504-36a43e3a0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our alpha for 95% confidence\n",
    "alpha = 0.05\n",
    "\n",
    "# length of the original dataset\n",
    "n = len(x)\n",
    "# degrees of freedom\n",
    "dof = n - 2\n",
    "\n",
    "# t-value for alpha/2 with n-2 degrees of freedom\n",
    "t = stats.t.ppf(1-alpha/2, dof)\n",
    "\n",
    "# compute the upper and lower limits on our B1 (slope) parameter\n",
    "B1_upper = B1 + t * sB1\n",
    "B1_lower = B1 - t * sB1\n",
    "\n",
    "# compute the corresponding upper and lower B0 values (y intercepts)\n",
    "B0_upper = y.mean() - B1_upper*x.mean()\n",
    "B0_lower = y.mean() - B1_lower*x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b4240-2186-4e34-8c0e-23a0e9bfd7a3",
   "metadata": {},
   "source": [
    "Create some predictions values, compute our error of prediction (sigma_ep) for each p_x, then the lower and upper confidence limits (for 95%) can be computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da9fda-e1f0-4aed-a22f-e71cb60f284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an array of x values\n",
    "p_x = np.linspace(x.min(),x.max(),100)\n",
    "\n",
    "# using our model parameters to predict y values\n",
    "p_y = B0 + B1*p_x\n",
    "\n",
    "# calculate the standard error of the predictions\n",
    "sigma_ep = np.sqrt( s**2 * (1 + 1/n + ( ( n*(p_x-x.mean())**2 ) / ( n*np.sum(x**2) - np.sum(x)**2 ) ) ) )\n",
    "\n",
    "# our chosen alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# compute our degrees of freedom with the length of the predicted dataset\n",
    "n_p = len(p_x)\n",
    "dof = n_p - 2\n",
    "\n",
    "# get the t-value for our alpha and degrees of freedom\n",
    "t = stats.t.ppf(1-alpha/2, dof)\n",
    "\n",
    "# compute the upper and lower limits at each of the p_x values\n",
    "p_y_lower = p_y - t * sigma_ep\n",
    "p_y_upper = p_y + t * sigma_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfccf6-8c4c-4f2e-a341-69a7f2c26c57",
   "metadata": {},
   "source": [
    "**Plot it all again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364fe08-848a-4d5d-8618-793090fecc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7), dpi=300, tight_layout=True)\n",
    "\n",
    "# Scatterplot of original data\n",
    "ax.scatter(x, y, c='k', label='Original Data')\n",
    "\n",
    "# Plot the mean line, we only need two points to define a line, use xmin and xmax\n",
    "ax.plot([x.min(), x.max()], [y.mean(), y.mean()] , '--m', label='Mean Y')\n",
    "\n",
    "# Plot the mean x line\n",
    "plt.axvline(x.mean(),c='k', linestyle='--', label='Mean X Value')\n",
    "\n",
    "# Plot the linear regression model\n",
    "ax.plot([x.min(), x.max()], [B0 + B1*x.min(), B0 + B1*x.max()], '-r', label='Least Squares Linear Regression Model')\n",
    "\n",
    "# Plot the upper and lower confidence limits for the standard error of the gradient (slope)\n",
    "ax.plot([x.min(), x.max()], [B0_upper + B1_upper*x.min(), B0_upper + B1_upper*x.max()] , '--r', label='Upper B0 confidence limit (95%)')\n",
    "ax.plot([x.min(), x.max()], [B0_lower + B1_lower*x.min(), B0_lower + B1_lower*x.max()] , '--r', label='Upper B0 confidence limit (95%)')\n",
    "\n",
    "# Plot confidence limits on our predicted Y values\n",
    "ax.plot(p_x, p_y_upper, ':b', label='Upper Y prediction interval (95%)')\n",
    "ax.plot(p_x, p_y_lower, ':b', label='Lower Y prediction interval (95%)')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='lower right');\n",
    "\n",
    "# Add axes labels and title\n",
    "ax.set_xlabel(\"Annual Mean Flow [cfs]\")\n",
    "ax.set_ylabel(\"Annual Peak Flow [cfs]\")\n",
    "ax.set_title('Flow Scatterplot');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a8ea3-b639-4603-8453-3bff48ef399f",
   "metadata": {},
   "source": [
    "## 2.5. Quantile Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69272f-3373-499a-9d3e-38009d639555",
   "metadata": {},
   "source": [
    "## Steps to create a quantile regression model:\n",
    "\n",
    "**1)** For each of your two datasets, create an empirical CDF\n",
    "\n",
    "We can do this with a custom function like the `cunnane_quantile_array()` function below, which gives us quantile values given an array of numbers.\n",
    "\n",
    "However, in this case, we want to be able to \"look up\" any quantile value (even those that lie between data points). For this, we can use `scipy.stats.mstats.mquantiles()` instead.\n",
    "\n",
    "Review the documentation for [scipy.stats.mstats.mquantiles](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.mquantiles.html), recall that the default options give us the Cunnane plotting position. Note how the function handles quantiles as they approach 0 or 1 at the lowest and highest end of our values. How many (quantile) values should we use in the input to this function to create an empirical CDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7e340-913d-42b7-9f67-fdf9f3ac9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should be able to accept any one-dimensional numpy array or list, of numbers\n",
    "# It returns two numpy arrays, one of the sorted numbers, the other of the plotting position\n",
    "def cunnane_quantile_array(numbers):\n",
    "    '''This function also computes the Cunnane plotting position given an array or list of numbers (rather than a pandas dataframe).\n",
    "    It has two outputs, first the sorted numbers, second the Cunnane plotting position for each of those numbers.\n",
    "    [Steven Pestana, spestana@uw.edu, Oct. 2020]'''\n",
    "    \n",
    "    # 1) sort the data, using the numpy sort function (np.sort())\n",
    "    sorted_numbers = np.sort(numbers)\n",
    "    \n",
    "    # length of the list of numbers\n",
    "    n = len(sorted_numbers) \n",
    "    \n",
    "    # make an empty array, of the same length. below we will add the plotting position values to this array\n",
    "    cunnane_plotting_position = np.empty(n)\n",
    "    \n",
    "    # 2) compute the Cunnane plotting position for each number, using a for loop and the enumerate function\n",
    "    for rank, number in enumerate(sorted_numbers):\n",
    "        cunnane_plotting_position[rank] = ( (rank+1) - (2/5) ) / ( n + (1/5) )\n",
    "    \n",
    "    return sorted_numbers, cunnane_plotting_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eebcb9-9d2d-4e45-9525-d510f005c085",
   "metadata": {},
   "source": [
    "We can create both types of quantile plots and look at them together. When building the quantile regression model, we'll use both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8bc27-5608-45b5-b506-3d0d64fae7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "\n",
    "# Here we use the actual values from the dataset to create the plots\n",
    "# BLC -> PF\n",
    "# SLI -> MF\n",
    "PF_ordered, PF_quantile = cunnane_quantile_array(st_law_flow_df['peakflow_cfs'])\n",
    "MF_ordered, MF_quantile = cunnane_quantile_array(st_law_flow_df['meanflow_cfs'])\n",
    "plt.plot(PF_ordered, PF_quantile, 'o', markeredgecolor='b', markerfacecolor='None', markersize=7, label='Annual Peak Flow Quantile Plot from observed values')\n",
    "plt.plot(MF_ordered, MF_quantile, 'o', markeredgecolor='r', markerfacecolor='None', markersize=7, label='Annual Mean Flow Quantile Plot from observed values')\n",
    "\n",
    "\n",
    "# We can also create these by picking arbitrary quantile values, then using the scipy.stats.mstats.mquantiles function\n",
    "quantiles = np.linspace(0,1,100) # 100 quantile values linearly spaced between 0 and 1\n",
    "plt.plot(stats.mstats.mquantiles(st_law_flow_df['peakflow_cfs'], quantiles), quantiles, \n",
    "         'b.', label='Annual Peak Flow Quantile Plot from interpolated probabilities', alpha=0.7)\n",
    "plt.plot(stats.mstats.mquantiles(st_law_flow_df['meanflow_cfs'], quantiles), quantiles, \n",
    "         'r.', label='Annual Mean Flow Quantile Plot from interpolated probabilities', alpha=0.7)\n",
    "\n",
    "plt.ylabel('Quantile')\n",
    "plt.xlabel('Flow (cfs)')\n",
    "# plt.xlim((0,2500))\n",
    "plt.ylim((0,1))\n",
    "plt.title('Quantiles of Flow data')\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7d8d1-9e33-4986-9877-c1a001bfa222",
   "metadata": {},
   "source": [
    "**2)** Use the two empirical CDFs as a way of looking-up (or mapping) values from the predictor to the predictand, by matching which physical value corresponds to the same quantile.\n",
    "\n",
    "The example below does this with one data point, where we start with a value of SWE at Slide Canyon, look up its quantile, then find the corresponding SWE value at Blue Canyon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72440e0-5286-4f6c-af7b-7736cc91a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will aslo need this 1d interpolation function\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# This is our empirical cdf of the Slide Canyon data, which also includes values down to 0 and up to 1.\n",
    "MF_quantile = np.linspace(0,1,100)\n",
    "MF_ordered = stats.mstats.mquantiles(st_law_flow_df['meanflow_cfs'], MF_quantile)\n",
    "\n",
    "# When Slide Canyon has SWE equal to it's median, how much snow can we expect at Blue Canyon?\n",
    "MF_test = st_law_flow_df['meanflow_cfs'].median()\n",
    "\n",
    "# Create a linear interpolation object based on these values (this lets us look up any value, x, and get back the y value)\n",
    "f_MF = interp1d(MF_ordered, MF_quantile)\n",
    "MF_test_quantile = f_MF(MF_test)\n",
    "\n",
    "print('In the empirical Annual Mean Flow CDF,'+ \n",
    "      'a value of {} cfs (the median) corresponds'.format(MF_test)+\n",
    "      'to a quantile of {}'.format(np.round(MF_test_quantile,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9072a0c-f4c4-42f0-9c77-bcda9062cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "\n",
    "# We can also create these by picking arbitrary quantile values, then using the scipy.stats.mstats.mquantiles function\n",
    "quantiles = np.linspace(0,1,100) # 100 quantile values linearly spaced between 0 and 1\n",
    "plt.plot(stats.mstats.mquantiles(st_law_flow_df['peakflow_cfs'], quantiles), quantiles, \n",
    "         'b.', label='Annual Peak Flow Quantile Plot from interpolated probabilities', alpha=0.7)\n",
    "plt.plot(stats.mstats.mquantiles(st_law_flow_df['meanflow_cfs'], quantiles), quantiles, \n",
    "         'r.', label='Annual Mean Quantile Plot from interpolated probabilities', alpha=0.7)\n",
    "\n",
    "# Plot the test point value\n",
    "plt.plot(MF_test,MF_test_quantile,'D', markerfacecolor='m', markeredgecolor='k',markersize=10, label='MF_test ({},{})'.format(MF_test, np.round(MF_test_quantile,2)))\n",
    "# Plot a line from the x-axis to the test point\n",
    "plt.plot([MF_test, MF_test], [0, MF_test_quantile], c='m', linestyle='-')\n",
    "# Plot a line from the test point to the y-axis\n",
    "plt.plot([0, MF_test], [MF_test_quantile, MF_test_quantile], c='k', linestyle='-')\n",
    "\n",
    "plt.ylabel('Quantile')\n",
    "plt.xlabel('Flow (cfs)')\n",
    "plt.xlim((180000,380000))\n",
    "plt.ylim((0,1))\n",
    "plt.title('Quantiles of Flow data')\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e1551-bf10-4a0a-9ca3-fb6567674fbe",
   "metadata": {},
   "source": [
    "We see that our test value corresponds to the median value at Slide Canyon, quantile value 0.5. \n",
    "\n",
    "(Yes, you would hope so, since I defined it as the median to begin with, but it's always best practice to start coding with a situation where you know the right answer.)\n",
    "\n",
    "Now, we need to take this Slide Canyon quantile value (0.5) and find the Blue Canyon SWE value that corresponds to its same quantile value (finding the Blue Canyon median in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc3067-078f-4774-bd73-924215ab64c0",
   "metadata": {},
   "source": [
    "We first need to create an interpolation object that lets us translate from Blue Canyon quantile values to Blue Canyon SWE values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b2f5f-3b12-4e00-ba46-259939b86486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our empirical cdf of the Blue Canyon data, which also includes values down to 0 and up to 1.\n",
    "PF_quantile = np.linspace(0,1,100)\n",
    "PF_ordered = stats.mstats.mquantiles(st_law_flow_df['peakflow_cfs'], PF_quantile)\n",
    "\n",
    "# Create a linear interpolation object based on these values (this lets us look up any value, y, and get back the x value) \n",
    "# *note we've reversed the order of quantiles and SWE compared the the first interpolation object we created\n",
    "g_PF = interp1d(PF_quantile, PF_ordered)\n",
    "\n",
    "# So if we look up a quantile value in our function g_BLC()\n",
    "PF_test = g_PF(MF_test_quantile)\n",
    "\n",
    "print('In the empirical Annual Peak Flow CDF,' +\n",
    "      'a quantile of {} corresponds'.format(np.round(MF_test_quantile,2))+\n",
    "      ' to a flow value of {} cfs (the median)'.format(PF_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3dd4e-0c28-44ad-bb13-fd556e9d78fa",
   "metadata": {},
   "source": [
    "Visualize the complete problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ecbfc-6985-4d68-aec2-905deebc15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "\n",
    "# We can also create these by picking arbitrary quantile values, then using the scipy.stats.mstats.mquantiles function\n",
    "quantiles = np.linspace(0,1,100) # 100 quantile values linearly spaced between 0 and 1\n",
    "plt.plot(stats.mstats.mquantiles(st_law_flow_df['peakflow_cfs'], quantiles), quantiles, \n",
    "         'b.', label='Annual Peak Flow Quantile Plot from interpolated probabilities', alpha=0.7)\n",
    "plt.plot(stats.mstats.mquantiles(st_law_flow_df['meanflow_cfs'], quantiles), quantiles, \n",
    "         'r.', label='Annual Mean Quantile Plot from interpolated probabilities', alpha=0.7)\n",
    "\n",
    "# Plot the test point value\n",
    "plt.plot(MF_test,MF_test_quantile,'D', markerfacecolor='m', markeredgecolor='k',markersize=10, label='MF_test ({},{})'.format(MF_test, np.round(MF_test_quantile,2)))\n",
    "# Plot a line from the x-axis to the test point\n",
    "plt.plot([MF_test, MF_test], [0, MF_test_quantile], c='m', linestyle='-')\n",
    "# Plot a line from the test point to the y-axis\n",
    "plt.plot([0, PF_test], [MF_test_quantile, MF_test_quantile], c='k', linestyle='-')\n",
    "\n",
    "# Plot the Blue Canyon test point value\n",
    "plt.plot(PF_test,MF_test_quantile,'D', markerfacecolor='c', markeredgecolor='k',markersize=10, label='MF_test ({},{})'.format(PF_test, np.round(MF_test_quantile,2)))\n",
    "# Plot a line from the test point to the x-axis\n",
    "plt.plot([PF_test, PF_test], [0, MF_test_quantile], c='c', linestyle='-')\n",
    "\n",
    "plt.ylabel('Quantile')\n",
    "plt.xlabel('Flow (cfs)')\n",
    "plt.xlim((180000,380000))\n",
    "plt.ylim((0,1))\n",
    "plt.title('Quantiles of Flow data')\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35831f1b-782e-48ec-b88d-854836acbcd7",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.5.1. Aplly to full dataset\n",
    "\n",
    "Now that we've walked through a single-point example, we can apply these steps efficiently to the whole dataset, starting from the beginning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3a47c-608b-40d7-8bfd-0522b3a27bf4",
   "metadata": {},
   "source": [
    "1) Create empirical CDFs for both data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ae710-4c91-453b-8ef2-90315c94cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.linspace(0,1,100)\n",
    "\n",
    "# This is our empirical cdf of the Slide Canyon data, which also includes values down to 0 and up to 1.\n",
    "MF_ordered = stats.mstats.mquantiles(st_law_flow_df['meanflow_cfs'], quantiles)\n",
    "\n",
    "# This is our empirical cdf of the Blue Canyon data, which also includes values down to 0 and up to 1.\n",
    "PF_ordered = stats.mstats.mquantiles(st_law_flow_df['peakflow_cfs'], quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6a86c-ce4b-4887-8f87-ed6d63b145f0",
   "metadata": {},
   "source": [
    "2) Use the CDFs to \"look up\" Annual Mean Flow to predict Annual Peak Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa621b-fda0-4fc0-8ad2-e2b95f55c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our interpolation function for looking up a quantile given a value of SWE at Slide Canyon\n",
    "f_MF = interp1d(MF_ordered, quantiles)\n",
    "# Create our interpolation function for looking up SWE at Blue Canyon given a quantile\n",
    "g_PF = interp1d(quantiles, PF_ordered)\n",
    "\n",
    "# Now, we can create a prediction for every value in the Slide Canyon dataset to come up with a matching prediction for the Blue Canyon dataset\n",
    "PF_predicted=g_PF( f_MF( st_law_flow_df['meanflow_cfs'] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d6b34-e3fe-46a8-ba28-b0033008e4f4",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85dd6d-56b7-477f-a18a-cae037ae7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can see how well this did by making a time series plot of our actual and predicted values\n",
    "# Original data:\n",
    "plt.figure(figsize=(10,5),dpi=300)\n",
    "plt.plot(st_law_flow_df['year'],st_law_flow_df['meanflow_cfs'],'b-', label='Annual Mean Flow');\n",
    "plt.plot(st_law_flow_df['year'],st_law_flow_df['peakflow_cfs'],'r-', label='Annual Peak Flow');\n",
    "\n",
    "# Predicted with linear regression between Slide Canyon and Blue Canyon\n",
    "plt.plot(st_law_flow_df['year'],PF_predicted,'k--', \n",
    "         label='Annual Peak Flow Predicted from Quantile Regression')\n",
    "plt.legend()\n",
    "plt.title('Timeline of Annual Peak Flow')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Flow [cfs]');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2db71-09b3-4b50-9d08-374765ebab4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
