{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b290235-8f87-4608-ab0f-ce381395eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ccb20-61bc-40b6-8367-ad386f23ff90",
   "metadata": {},
   "source": [
    "# Python inclass practice 6: Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e78ce-45d2-476b-a7b1-b1c9ddca315a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. First, let's revisit how to download USGS data!\n",
    "### 1.1. Define site-specific info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3392479-7985-4c6e-b5ab-60261c8dd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'site_no': '04216000',\n",
    "    'begin_date': '1930-09-01',\n",
    "    'end_date': '2023-08-31'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62918e8-f009-40a3-9845-d7d26de90b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = urllib.parse.urlencode(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161e8a0-8a66-46c4-b5b9-f3d3c1eb2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b263674-0df7-49d8-88b1-b582b27ef3a0",
   "metadata": {},
   "source": [
    "## 1.2. Create the url and access the data using urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1746df-fad1-4c09-b58c-fe762d95e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "verde_url = (\n",
    "    f'https://waterdata.usgs.gov/nwis/dv?'\n",
    "    f'cb_00060=on&format=rdb&referred_module=sw&{query}'\n",
    ")\n",
    "print(verde_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550f751-d7dd-4c68-a418-4eba7a2c6068",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3. Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e9966-c2ab-47cc-b9f1-7315e08b5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib.request.urlopen(verde_url)\n",
    "\n",
    "# Anyways, let's walk through a few of them:\n",
    "#  - comment='#': Lines beginning with a '#' are comments that pandas should ignore\n",
    "#  - sep='\\s+': The data representing columns are separated by white space\n",
    "#  - names: The names of the columns. I set these because the USGS ones are trash\n",
    "#  - index_col=2: Set the 3rd column as the index (that is, \"date\")\n",
    "#  - parse_dates=True: Try to make dates the correct data type, didn't work here but a good idea\n",
    "#  - date_format='yyyy-mm-dd': Display the format of date\n",
    "#  - engine='python': Python engine is currently more feature-complete\n",
    "\n",
    "df = pd.read_table(\n",
    "    response,\n",
    "    comment='#',\n",
    "    sep='\\s+',\n",
    "    names=['agency', 'site', 'date', 'streamflow', 'quality_flag'],\n",
    "    index_col=2,\n",
    "    parse_dates=True,\n",
    "    date_format='yyyy-mm-dd',\n",
    "    engine='python'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41aaa70-d9a3-4308-a108-f72f612aae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard the first two rows\n",
    "df = df.iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9d8c4-d55f-4493-a582-89b996ac036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert the streamflow data to floats and\n",
    "# the index to datetimes. When processing raw data\n",
    "# it's common to have to do some extra postprocessing\n",
    "df['streamflow'] = df['streamflow'].astype(np.float64)\n",
    "df.index = pd.DatetimeIndex(df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995bacd9-c29a-4dc8-aaaf-3f90f97eb456",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4. Practice #1: Please download following dataset\n",
    "Site id: 04215000 (Cayuga Creek neear Lancaster NY) </br>\n",
    "Start date: 1930-09-01 </br>\n",
    "End date: 2023-08-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33843e88-93d4-4d3c-a504-4395647344f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT your code here\n",
    "# save data for Cayuga sites to **df1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88f6fd-fc7c-4933-b873-3a8e9da00f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.5. Concatenate multi-site information\n",
    "\n",
    "First, we only want streamflow data,so when concatenating multi-site information, we only only keep the column \"streamflow\". However, if both sites use \"streamflow\" as their column names, how can we tell which is which? We also need to rename the column names to their USGS site ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9fb6e-3492-4826-b171-620f74fdbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = df[['streamflow']].rename({'streamflow':'04216000'},axis=1)\n",
    "df_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d82db-b493-4d75-bd47-c8baefb91596",
   "metadata": {},
   "source": [
    "#### 1.5.0. Practice #2: Like we did to `df`, extract \"streamflow\" column from `df1`, and rename the column \"streamflow\" to its corresponding USGS site ID \"04215000\"\n",
    "\n",
    "##### _n represents Niagara River and _c represents Cayuga Creek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3161593-a272-4e85-a34a-74c6c6fab03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = #INSERT your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70e4f6-8658-49d5-8371-5072e1395d69",
   "metadata": {},
   "source": [
    "### 1.5.1. concatenate two dataframes `df_n` and `df_c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9ad09-bc77-4989-aeb9-d8bcad90b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_n,df_c],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f0589-a52b-4778-8e7d-d00d2c0d51b0",
   "metadata": {},
   "source": [
    "### 1.5.2. We can quickly take a look at the two time series by simply using `df.plot`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c2660-cfa3-406e-be9b-185cbe33f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f4092-ed9e-4556-ae80-9610e5035efc",
   "metadata": {},
   "source": [
    "### 1.5.3. Practice #3: We noticed that there is some missing data from the late 1960s to early 1970s. For simplicity, we will focus on the period when both sites have data. Let's select the data between `1980-01-01` and `2022-12-31`.\n",
    "If you don't remember how to select time series, check Section **2.4.2** in https://github.com/act-hydro/GLY606_2024/blob/main/in_class_practice/python_practice/python_inclass_5_dataframe.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22ff7b-ffea-4a87-8a82-3e8828235826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_sel = #Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878432c8-f1c6-4b24-bfd0-c617d57e8408",
   "metadata": {},
   "source": [
    "### 1.5.4. Last sanity check: we do not want have any missing data in the dataset\n",
    "`df.dropna()` will automatically drop the rows with missing values. </br>\n",
    "If we want to drop the columns with missing values, we should use `df.dropna(axix=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51b9ba-7ab6-46a8-8c61-fc094f0ede46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_sel = df_concat_sel.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aacf430-183e-4465-b11a-94aefc3a6707",
   "metadata": {},
   "source": [
    "# 2. Statistics calcluations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f9d00-b27e-4d10-95b4-4f382fc8cee4",
   "metadata": {},
   "source": [
    "## 2.1. Mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d594d03-8036-4afc-b8f9-65e9d4b10002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_sel = pd.read_csv(\"python_inclass_practice_6.data.csv\",index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aad33d4-5f0e-49ec-ac97-b739885f5258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>04216000</th>\n",
       "      <th>04215000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-01-01</th>\n",
       "      <td>227000.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-02</th>\n",
       "      <td>230000.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-03</th>\n",
       "      <td>223000.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-04</th>\n",
       "      <td>214000.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-05</th>\n",
       "      <td>214000.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>213000.0</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>209000.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>206000.0</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>207000.0</td>\n",
       "      <td>1570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>211000.0</td>\n",
       "      <td>1170.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15705 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            04216000  04215000\n",
       "date                          \n",
       "1980-01-01  227000.0     138.0\n",
       "1980-01-02  230000.0     123.0\n",
       "1980-01-03  223000.0     102.0\n",
       "1980-01-04  214000.0      51.0\n",
       "1980-01-05  214000.0      45.0\n",
       "...              ...       ...\n",
       "2022-12-27  213000.0     142.0\n",
       "2022-12-28  209000.0     182.0\n",
       "2022-12-29  206000.0     278.0\n",
       "2022-12-30  207000.0    1570.0\n",
       "2022-12-31  211000.0    1170.0\n",
       "\n",
       "[15705 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat_sel.iloc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb9897-41ad-4cb1-8854-772c40856395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_mean = df_concat_sel.mean()\n",
    "df_concat_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08012c7b-3ab9-4d37-a62b-4416e805d84c",
   "metadata": {},
   "source": [
    "### 2.1.1. How do we quickly visualize the mean values in a time series plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993e6cb-ec57-41ed-a8d2-a9bedc2b1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=[5,3],dpi=300)\n",
    "df_concat_sel[['04216000']].plot(ax=ax,lw=0.5)\n",
    "ax.set_title('USGS streamflow @ Niagara River (Buffalo, NY)\\n Site No 04216000')\n",
    "ax.set_ylabel('streamflow [cfs]')\n",
    "ax.axhline(df_concat_mean['04216000'],c='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc394165-2ea0-4720-9912-97b58dab11c9",
   "metadata": {},
   "source": [
    "### 2.1.2. Practice #4: A quick recap of subplots!\n",
    "Since Niagara River and Cayuga Creek near Lancaster NY have different flow volumns, it is preferrable to plot them separately. So let's use `plt.subplots` to show the time series and mean values for both sites. </br>\n",
    "\n",
    "If you forgot how subplots works, check the bottom part of inclass practice 3 https://github.com/act-hydro/GLY606_2024/blob/main/in_class_practice/python_practice/python_inclass_3_intro_to_matplotlib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8caf6-62c1-4401-98a0-1240af2924b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please insert your code below "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a3765-b94b-4f87-96f8-9b45b0de5aa0",
   "metadata": {},
   "source": [
    "### 2.1.3. What we can learn from the time series plots above?\n",
    "Drainage area for Niagara River in Buffalo NY: 263,700 square miles </br>\n",
    "Drainage area for Cayuga Creek near Landcaster NY: 96.4 square miles </br>\n",
    "\n",
    "Streamflow in smaller river basins like Cayuga Creek is usually more responsive to precipitation events, leading to a more spiky high flow events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc3ce0-a3c8-4339-b5fb-e90e8679bd3b",
   "metadata": {},
   "source": [
    "## 2.2. standard deviation (SD)\n",
    "To calculate the standard deviation for a column in dataframe, we can simply use `df.std()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ccc21f-fd34-4567-be38-b294a3bc87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_sd = df_concat_sel.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c373e96-39ea-4267-9bb2-0b0728794314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db79b6c-64e7-4ad1-93b1-1e23b47cc0b7",
   "metadata": {},
   "source": [
    "## 2.3. Coefficient of variation\n",
    "CV = (Standard Deviation) / (Mean) </br> \n",
    "A higher CV indicates greater relative variability in the data, meaning the data points are spread further apart compared to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a603c2-9514-43d9-94db-be069f2ff224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_coeff_variation = df_concat_sd / df_concat_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ede71-a039-479b-ab85-e94ddc59bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_coeff_variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00858cd-6df7-4540-a2be-89b17e5f7cfa",
   "metadata": {},
   "source": [
    "### Once again, Cayuga Creek has a much higher coefficient of variation than Niagara River, which echos our observations about the streamflow timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba800cb-7235-480a-a2e5-82f1918d960f",
   "metadata": {},
   "source": [
    "## 2.4. Box plots\n",
    "\n",
    "We can use box plots to show the spread of the datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24016a05-8ec0-4c26-b46b-82af8374d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2,figsize=[5,6],dpi=100)\n",
    "axes[0].boxplot(df_concat_sel[['04216000']])\n",
    "axes[0].set_ylabel(\"streamflow [cfs]\")\n",
    "axes[0].set_xticks([1],['0421600'])\n",
    "\n",
    "axes[1].boxplot(df_concat_sel[['04215000']])\n",
    "axes[1].set_xticks([1],['0421500'])\n",
    "axes[1].set_ylim(0,)\n",
    "# axes[1].set_yscale('log')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb8e9a-c1b6-4621-afb1-9226ff78f70b",
   "metadata": {},
   "source": [
    "### 2.4.1. Practice #5: log-scale of y-axis\n",
    "The box plot in the Cayuga River is largely influenced by the high flow events. What if we change the y-axis to log-scale? Please uncomment `ax.set_yscale('log')` and see what changes this line makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca43732-fd96-47b4-9207-47f62e0d0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=[6,10],dpi=300)\n",
    "ax.boxplot(df_concat_sel[['04215000']],flierprops={'marker':'+','linewidth':0.2})\n",
    "ax.set_xticks([1],['0421500'])\n",
    "# ax.set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02411753-b0be-4c99-bf32-f1a38be585a8",
   "metadata": {},
   "source": [
    "# 3. PDF & CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248ef4e-6f95-42e4-ab6b-96dfbca46d78",
   "metadata": {},
   "source": [
    "## 3.1. Probability Density Function\n",
    "\n",
    "For discrete sample data, we usually use histogram `plt.hist` to visualize it. </br>\n",
    "Note: this plotting function has an option called `density`. If `density=False`, the plot will show the count falling within each bin. If `density=True`, the plot will show the probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baa540-5291-4669-bab9-87598ba65818",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1,figsize=[5,4],dpi=200)\n",
    "ax.hist(df_concat_sel['04216000'],bins=np.arange(100000,350000,10000),density=True)\n",
    "ax.set_title(\"PDF for streamflow in Niagara River\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c872134-2bd5-468e-b7e5-51c67ea64e00",
   "metadata": {},
   "source": [
    "### 3.1.1. It is important to choose bin size and scales wisely\n",
    "\n",
    "For example, the streamflow regime in Cayuga River is very spiky. Most of the time, the flow is between 0-100 cfs, but it also had some very high flow days with thousands cfs. Therefore, if we use normal scale plots with fixed bins, it will look like left plot below. However, if we use log-scale for x-axis with smart selection of bin width, the plot looks much better (like the one on the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475903f5-6ebc-4b79-860c-6d68e9b55bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1, 2,figsize=[10,4],dpi=200)\n",
    "\n",
    "axes[0].hist(df_concat_sel['04215000'],bins=np.arange(100,4000,100),\n",
    "             density=True)\n",
    "axes[1].hist(df_concat_sel['04215000'],bins=np.concatenate([[0,1,2,5,10,20,50,80],\n",
    "                                                            np.arange(100,4000,100)]),\n",
    "             density=True)\n",
    "axes[1].set_xscale('log')\n",
    "axes[0].set_title(\"Linear-scale\")\n",
    "axes[1].set_title(\"Log-scale\")\n",
    "\n",
    "fig.suptitle(\"PDF for streamflow in Cayuga Creek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8aeeaf-d2aa-4830-85c5-61f563cee11c",
   "metadata": {},
   "source": [
    "## 3.2. Cumulative Distribution Function\n",
    "\n",
    "If we have a discrete array of samples, and we would like to know the CDF of the sample, then we can just sort the array. If we look at the sorted result, we'll realize that the smallest value represents 0% , and largest value represents 100 %. Then all other arrays are assigned with evenly distributed probabilities between 0% and 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530e73d-6f5b-4607-94b7-d440e9c41308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative proportion of the data that falls below each value\n",
    "cumulative = np.linspace(0, 1, len(df_concat_sel['04216000']))\n",
    "\n",
    "# Sort the data in ascending order\n",
    "sorted_data = np.sort(df_concat_sel['04216000'])\n",
    "\n",
    "# Calculate the cumulative proportion of the sorted data\n",
    "cumulative_data = np.cumsum(sorted_data) / np.sum(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a8a75-67b4-4fb2-b040-eca3993ffed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the CDF\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(sorted_data, cumulative_data, label='streamflow')\n",
    "plt.ylim(0,1)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Streamflow (cfs)\")\n",
    "plt.ylabel(\"Cumulative Proportion\")\n",
    "plt.title(\"Cumulative Distribution Function (CDF) of Mean daily flow\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9203b30-5770-45b3-9afb-58d3e7c14adb",
   "metadata": {},
   "source": [
    "## 3.2.1. Practice #6: plot CDF for Cayuga River. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925ffa4-2f37-4acb-ba94-330c5887c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc7cab-38bd-40e6-86cc-0656d37e584f",
   "metadata": {},
   "source": [
    "## 3.3. 100-year return floods\n",
    "\n",
    "As we discussed in class, to calculate the 100-year return floods, we can follow the three steps:\n",
    "1. Identify annual peak flow `groupby`\n",
    "2. Calculate the CDF for annual peak flow\n",
    "3. Calculate the Annual exceedance probability (AEP)\n",
    "4. In the AEP, find the flow value corresponds to AEP = 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cd7d1-ae8b-499e-91ef-3358f88f110a",
   "metadata": {},
   "source": [
    "#### 3.3.1. Step 1: Identify annual peak flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f6b84-b183-49b5-8e8f-a454ac4a7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify annual peak flow\n",
    "df_annual_max = df_concat_sel.groupby(df_concat_sel.index.year).max()\n",
    "df_annual_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c9f12-05e4-45e6-93a1-6514f73e27c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.3.2. calculate cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7e592-5359-437a-9135-c4e4d2cfd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: calculate cdf\n",
    "# Calculate the cumulative proportion of the data that falls below each value\n",
    "cumulative_n = np.linspace(0, 1, len(df_annual_max['04216000']))\n",
    "\n",
    "# Sort the data in ascending order\n",
    "sorted_data_n = np.sort(df_annual_max['04216000'])\n",
    "\n",
    "# Calculate the cumulative proportion of the sorted data\n",
    "cumulative_data_n = np.cumsum(sorted_data_n) / np.sum(sorted_data_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657dea5-60e9-4773-ad24-c81dc2d1e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_data_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d42950-4b08-4271-a466-59bc3abc9e9b",
   "metadata": {},
   "source": [
    "#### 3.3.3. calculate AEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9084c15-d2a9-45cc-b6cc-4de335fab2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: calculate AEP\n",
    "aep_n = 1 - cumulative_data_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05079109-5ade-43f4-b822-37b9d908fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize AEP\n",
    "plt.figure(dpi=200)\n",
    "plt.plot(aep_n, sorted_data_n,  label='Niagara River Peak Flow')\n",
    "plt.scatter(aep_n, sorted_data_n)\n",
    "plt.axvline(0.01,lw=0.5,c='orange')\n",
    "plt.xlim(0,1)\n",
    "aep_list = np.array([0.01,0.05,0.1,0.2,0.5,1])\n",
    "aep_str = []\n",
    "for i in aep_list:\n",
    "    if i*100<1:\n",
    "        aep_str.append(\"%s%%\"%(i*100))\n",
    "    else:\n",
    "        aep_str.append(\"%i%%\"%(i*100))\n",
    "plt.xticks(aep_list,aep_str)\n",
    "plt.grid(lw=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel(\"Streamflow (cfs)\")\n",
    "plt.xlabel(\"Annual Exceedance Probability\")\n",
    "plt.title(\"Annual Exceedance Probability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad5472-211c-4de3-91c6-6c5d33ceeed3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.3.4. find the flow corresponds to AEP = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df9917-44cd-4286-8cd6-1f30a17fab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: find the flow corresponds to AEP = 0.01\n",
    "\n",
    "# Basically, we need to find the y value that \n",
    "# corresponds to the intersection of orange line \n",
    "# and blue dotted line above\n",
    "\n",
    "# asp_n[::-1] reverse this array, for example\n",
    "# a = [0,1,2], and a[::-1] will output [2,1,0]\n",
    "\n",
    "# The reason behind reverse the asp_n is\n",
    "# np.interp requires an monotonically increasing \n",
    "# sample points but our asp_n was in monotonically\n",
    "# decreasing order.\n",
    "\n",
    "flow_100yr_flood_n = np.interp(0.01, aep_n[::-1], sorted_data_n[::-1])\n",
    "print(\"The flood with 100-year recurrence interval is %.2f cfs\"%(flow_100yr_flood_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160ca38-aba3-437f-bd23-ea4c544da041",
   "metadata": {},
   "source": [
    "## 3.4.1. Practice #7: Please output the 100-year flood for Cayuga River"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2036167-61c5-4604-8347-d2ae2c311537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb45d0-e021-488e-a1a3-e6bb71d2867e",
   "metadata": {},
   "source": [
    "## 3.5. 7Q10 for evaluating low flows!\n",
    "### 3.5.1. We first need to calculate the minimum annual 7-day average flow\n",
    "The syntax for rolling average is quite simple! </br>\n",
    "We can use `df.rolling()` to calculate the rolling average. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205491f-0d38-4426-9a89-dd95bb683bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolling_7day_mean = df_concat_sel.rolling(7,center=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038a66b-73a1-4a9d-aa66-69c0209d960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolling_7day_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876924d-bd69-4114-9020-7448c2592aa4",
   "metadata": {},
   "source": [
    "# Note: the first three days and last three days does not have data!\n",
    "Because data was not available for the nearby 7-days at those dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309332f7-0704-4754-826d-d581fea55cb0",
   "metadata": {},
   "source": [
    "### 3.5.2. Here we simply drop the NAN in the moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756d36b-a4cb-4223-be61-e7c58c84028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolling_7day_mean = df_rolling_7day_mean.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b108d-38c5-4b8d-b43f-9df75b887e49",
   "metadata": {},
   "source": [
    "### 3.5.3. Practice #8: Can you calculate the 7Q10 for Niagara River?\n",
    "This is definitely a harder exercise! However, the philosophy behind the calculation is very similar to how we calculate the annual peak flow with 100-year recurrence interval.\n",
    "\n",
    "7Q10 is the 7-day average annual low flow with 10-year recurrence interval.\n",
    "\n",
    "1. Calculate the 7-day average flow (we have done in **3.5.1/3.5.2**)\n",
    "2. Find the annual minimum 7-day average flow (use groupby, see Section **3.3.1**)\n",
    "3. Calculate CDF, which equals to Annual Non-exceedance Probability (ANEP)!!\n",
    "4. Find the flow corresponds to ANEP = 0.1 (which corresponds to 10-year recurrence interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc970171-287c-4ace-813a-f9ba988a0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify annual 7-day low flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69e955-ae53-4fc9-98d4-10f04321bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: calculate cdf, which equals to anep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0abd4b-d1d9-4290-97f9-e2828c5c75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (optional): visualize ANEP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c046fd-829f-42fd-9862-430f510d044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: find the flow corresponds to ANEP = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4543c-86d9-4bc8-b824-fa05b966f65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
